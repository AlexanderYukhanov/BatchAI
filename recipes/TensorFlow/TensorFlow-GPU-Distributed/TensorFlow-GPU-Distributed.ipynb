{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Tensorflow GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrate how to run standard TensorFlow sample (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) on Azure Batch AI cluster of 2 nodes.\n",
    "\n",
    "## Details\n",
    "\n",
    "- For demonstration purposes, MNIST dataset and mnist_replica.py will be deployed at Azure File Share;\n",
    "- Standard output of the job will be stored on Azure File Share;\n",
    "- MNIST dataset (http://yann.lecun.com/exdb/mnist/) is archived and uploaded into the blob https://batchaisamples.blob.core.windows.net/samples/mnist_dataset_original.zip?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=b&sig=Qc1RA3zsXIP4oeioXutkL1PXIrHJO0pHJlppS2rID3I%3D.\n",
    "- The recipe uses official [mnist_replica.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py) script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from azure.storage.file import FileService, FilePermissions\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions used by different notebooks\n",
    "sys.path.append('..\\..')\n",
    "import utilities\n",
    "\n",
    "cfg = utilities.Configuration('..\\..\\configuration.json')\n",
    "client = utilities.create_batchai_client(cfg)\n",
    "utilities.create_resource_group(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "\n",
    "- For this example we will use a gpu cluster of 2 `STANDARD_NC6` nodes. You can increase the number of nodes by changing `nodes_count` variable;\n",
    "- We will mount file share at folder with name `external`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/external`;\n",
    "- We will call the cluster `nc6`;\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_file_share = 'external'\n",
    "nodes_count = 2\n",
    "cluster_name = 'nc6'\n",
    "\n",
    "volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url = 'https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share)\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size=\"STANDARD_NC6\",\n",
    "    virtual_machine_configuration=models.VirtualMachineConfiguration(\n",
    "        image_reference=models.ImageReference(\n",
    "            publisher=\"microsoft-ads\",\n",
    "            offer=\"linux-data-science-vm-ubuntu\",\n",
    "            sku=\"linuxdsvmubuntu\",\n",
    "            version=\"latest\")),\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        mount_volumes=volumes,\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cluster_name, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "Monitor the just created cluster. utilities.py contains a helper function to print out detail status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Target: 2; Allocated: 2; Idle: 2; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n"
     ]
    }
   ],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deploy MNIST Dataset\n",
    "\n",
    "For demonstration purposes, we will download preprocessed MNIST dataset to the current directory and upload it to file share directory named `mnist_dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Extract MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://batchaisamples.blob.core.windows.net/samples/mnist_dataset_original.zip?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=b&sig=Qc1RA3zsXIP4oeioXutkL1PXIrHJO0pHJlppS2rID3I%3D ...Done\n",
      "Extracting MNIST dataset...Done\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset_url = 'https://batchaisamples.blob.core.windows.net/samples/mnist_dataset_original.zip?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=b&sig=Qc1RA3zsXIP4oeioXutkL1PXIrHJO0pHJlppS2rID3I%3D'\n",
    "mnist_files = ['t10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz',\n",
    "              'train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz']\n",
    "if any(not os.path.exists(f) for f in mnist_files):\n",
    "    utilities.download_file(mnist_dataset_url, 'mnist_dataset_original.zip')\n",
    "    print('Extracting MNIST dataset...', end='')\n",
    "    with zipfile.ZipFile('mnist_dataset_original.zip', 'r') as z:\n",
    "        z.extractall('.')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create File Share and Upload MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist_dataset_directory = 'mnist_dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to create folders and upload files into Azure File Share - you can use [Azure Portal](https://ms.portal.azure.com), [Storage Explorer](http://storageexplorer.com/), [Azure CLI2](/azure-cli-extension) or Azure SDK for your preferable programming language.\n",
    "In this example we will use Azure SDK for python to copy files into file share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, mnist_dataset_directory,\n",
    "    fail_on_exist=False)\n",
    "# Since uploading can take significant time, let's check first if the\n",
    "# file has been uploaded already.\n",
    "for f in mnist_files:\n",
    "    if service.exists(azure_file_share_name, mnist_dataset_directory, f):\n",
    "        continue\n",
    "    service.create_file_from_path(\n",
    "        azure_file_share_name, mnist_dataset_directory, f, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script and Configure the Input Directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_script_directory = 'tensorflow_samples'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each job we will create a folder containing a copy of the sample script. This allows to run the same job with different scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, mnist_script_directory, fail_on_exist=False)\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, mnist_script_directory, 'mnist_replica.py', 'mnist_replica.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job needs to know where to find ConvNet_MNIST.py and input MNIST dataset. We will create two input directories for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPT',\n",
    "        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, mnist_script_directory)),\n",
    "    models.InputDirectory(\n",
    "        id='DATASET',\n",
    "        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, mnist_dataset_directory))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job will be able to reference those directories using ```$AZ_BATCHAI_INPUT_SCRIPT``` and ```$AZ_BATCHAI_INPUT_DATASET``` environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Output Directories\n",
    "We will store standard and error output of the job in File Share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_output_path_prefix = \"$AZ_BATCHAI_MOUNT_ROOT/{0}\".format(azure_file_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "\n",
    "- The job will use `tensorflow/tensorflow:1.1.0-gpu` container.\n",
    "- Will use configured previously input and output directories.\n",
    "- Will use BatchAI reserved environment variable AZ_BATCHAI_TASK_INDEX to identify local task\n",
    "- By removing container_settings, the job will be ran on the host VMs if you are using DSVM\n",
    "\n",
    "**Note** You must agree to the following licences before using this container:\n",
    "- [TensorFlow License](https://github.com/tensorflow/tensorflow/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args_fmt = '--job_name={0} --num_gpus=1 --train_steps 1000 --ps_hosts=$AZ_BATCHAI_PS_HOSTS --worker_hosts=$AZ_BATCHAI_WORKER_HOSTS --task_index=$AZ_BATCHAI_TASK_INDEX --data_dir=$AZ_BATCHAI_INPUT_DATASET'\n",
    "job_name = datetime.utcnow().strftime(\"tf_%m_%d_%Y_%H%M%S\")\n",
    "parameters = models.job_create_parameters.JobCreateParameters(\n",
    "     location=cfg.location,\n",
    "     cluster=models.ResourceId(cluster.id),\n",
    "     node_count=2,\n",
    "     input_directories=input_directories,\n",
    "     std_out_err_path_prefix=std_output_path_prefix,\n",
    "     container_settings=models.ContainerSettings(\n",
    "         models.ImageSourceRegistry(image='tensorflow/tensorflow:1.1.0-gpu')),\n",
    "     tensor_flow_settings=models.TensorFlowSettings(\n",
    "         parameter_server_count=1,\n",
    "         worker_count=nodes_count,\n",
    "         python_script_file_path='$AZ_BATCHAI_INPUT_SCRIPT/mnist_replica.py',\n",
    "         master_command_line_args=args_fmt.format('worker'),\n",
    "         worker_command_line_args=args_fmt.format('worker'),\n",
    "         parameter_server_command_line_args=args_fmt.format('ps'),\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Job: tf_10_08_2017_074801\n"
     ]
    }
   ],
   "source": [
    "_ = client.jobs.create(cfg.resource_group, job_name, parameters)   \n",
    "print('Created Job: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enought idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stdeout-0.txt (the output of the worker running on the first node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Target: 2; Allocated: 2; Idle: 0; Unusable: 0; Running: 2; Preparing: 0; Leaving: 0\n",
      "Job state: running ExitCode: None\n",
      "Waiting for job output to become available...\n",
      "Extracting /mnt/batch/tasks/shared/LS_root/mounts/external/mnist_dataset/train-images-idx3-ubyte.gz\n",
      "Extracting /mnt/batch/tasks/shared/LS_root/mounts/external/mnist_dataset/train-labels-idx1-ubyte.gz\n",
      "Extracting /mnt/batch/tasks/shared/LS_root/mounts/external/mnist_dataset/t10k-images-idx3-ubyte.gz\n",
      "Extracting /mnt/batch/tasks/shared/LS_root/mounts/external/mnist_dataset/t10k-labels-idx1-ubyte.gz\n",
      "job name = worker\n",
      "task index = 0\n",
      "Worker 0: Initializing session...\n",
      "Worker 0: Session initialization complete.\n",
      "Training begins @ 1507448915.535585\n",
      "1507448915.730590: Worker 0: training step 1 done (global step: 0)\n",
      "1507448915.735376: Worker 0: training step 2 done (global step: 1)\n",
      "1507448915.738424: Worker 0: training step 3 done (global step: 2)\n",
      "1507448915.741825: Worker 0: training step 4 done (global step: 3)\n",
      "1507448915.745109: Worker 0: training step 5 done (global step: 4)\n",
      "1507448915.748233: Worker 0: training step 6 done (global step: 5)\n",
      "1507448915.755445: Worker 0: training step 7 done (global step: 6)\n",
      "1507448915.772454: Worker 0: training step 8 done (global step: 7)\n",
      "1507448915.777424: Worker 0: training step 9 done (global step: 8)\n",
      "1507448915.781015: Worker 0: training step 10 done (global step: 9)\n",
      "1507448915.784199: Worker 0: training step 11 done (global step: 10)\n",
      "1507448915.787456: Worker 0: training step 12 done (global step: 11)\n",
      "1507448915.790989: Worker 0: training step 13 done (global step: 12)\n",
      "1507448915.794385: Worker 0: training step 14 done (global step: 13)\n",
      "1507448915.797696: Worker 0: training step 15 done (global step: 14)\n",
      "1507448915.801160: Worker 0: training step 16 done (global step: 15)\n",
      "1507448915.812315: Worker 0: training step 17 done (global step: 16)\n",
      "1507448915.815435: Worker 0: training step 18 done (global step: 17)\n",
      "1507448915.818688: Worker 0: training step 19 done (global step: 18)\n",
      "1507448915.832274: Worker 0: training step 20 done (global step: 19)\n",
      "1507448915.835478: Worker 0: training step 21 done (global step: 20)\n",
      "1507448915.852450: Worker 0: training step 22 done (global step: 21)\n",
      "1507448915.857214: Worker 0: training step 23 done (global step: 22)\n",
      "1507448915.860902: Worker 0: training step 24 done (global step: 23)\n",
      "1507448915.874892: Worker 0: training step 25 done (global step: 24)\n",
      "1507448915.878058: Worker 0: training step 26 done (global step: 25)\n",
      "1507448915.882513: Worker 0: training step 27 done (global step: 26)\n",
      "1507448915.886103: Worker 0: training step 28 done (global step: 27)\n",
      "1507448915.889335: Worker 0: training step 29 done (global step: 28)\n",
      "1507448915.893749: Worker 0: training step 30 done (global step: 29)\n",
      "1507448915.896753: Worker 0: training step 31 done (global step: 30)\n",
      "1507448915.899972: Worker 0: training step 32 done (global step: 31)\n",
      "1507448915.908308: Worker 0: training step 33 done (global step: 32)\n",
      "1507448915.911654: Worker 0: training step 34 done (global step: 33)\n",
      "1507448915.914881: Worker 0: training step 35 done (global step: 34)\n",
      "1507448915.918856: Worker 0: training step 36 done (global step: 35)\n",
      "1507448915.921984: Worker 0: training step 37 done (global step: 36)\n",
      "1507448915.932880: Worker 0: training step 38 done (global step: 37)\n",
      "1507448915.936223: Worker 0: training step 39 done (global step: 38)\n",
      "1507448915.939513: Worker 0: training step 40 done (global step: 39)\n",
      "1507448915.942778: Worker 0: training step 41 done (global step: 40)\n",
      "1507448915.946049: Worker 0: training step 42 done (global step: 41)\n",
      "1507448915.949322: Worker 0: training step 43 done (global step: 42)\n",
      "1507448915.952576: Worker 0: training step 44 done (global step: 43)\n",
      "1507448915.956164: Worker 0: training step 45 done (global step: 44)\n",
      "1507448915.959538: Worker 0: training step 46 done (global step: 45)\n",
      "1507448915.962695: Worker 0: training step 47 done (global step: 46)\n",
      "1507448915.966034: Worker 0: training step 48 done (global step: 47)\n",
      "1507448915.969242: Worker 0: training step 49 done (global step: 48)\n",
      "1507448915.972452: Worker 0: training step 50 done (global step: 49)\n",
      "1507448915.975587: Worker 0: training step 51 done (global step: 50)\n",
      "1507448915.979505: Worker 0: training step 52 done (global step: 51)\n",
      "1507448915.982992: Worker 0: training step 53 done (global step: 52)\n",
      "1507448915.986258: Worker 0: training step 54 done (global step: 53)\n",
      "1507448915.989359: Worker 0: training step 55 done (global step: 54)\n",
      "1507448915.992595: Worker 0: training step 56 done (global step: 55)\n",
      "1507448915.996801: Worker 0: training step 57 done (global step: 56)\n",
      "1507448916.000168: Worker 0: training step 58 done (global step: 57)\n",
      "1507448916.003408: Worker 0: training step 59 done (global step: 58)\n",
      "1507448916.006585: Worker 0: training step 60 done (global step: 59)\n",
      "1507448916.010368: Worker 0: training step 61 done (global step: 60)\n",
      "1507448916.013502: Worker 0: training step 62 done (global step: 61)\n",
      "1507448916.016662: Worker 0: training step 63 done (global step: 62)\n",
      "1507448916.019732: Worker 0: training step 64 done (global step: 63)\n",
      "1507448916.022941: Worker 0: training step 65 done (global step: 64)\n",
      "1507448916.026099: Worker 0: training step 66 done (global step: 65)\n",
      "1507448916.029581: Worker 0: training step 67 done (global step: 66)\n",
      "1507448916.033529: Worker 0: training step 68 done (global step: 67)\n",
      "1507448916.040037: Worker 0: training step 69 done (global step: 68)\n",
      "1507448916.043508: Worker 0: training step 70 done (global step: 69)\n",
      "1507448916.046896: Worker 0: training step 71 done (global step: 70)\n",
      "1507448916.050337: Worker 0: training step 72 done (global step: 71)\n",
      "1507448916.053896: Worker 0: training step 73 done (global step: 72)\n",
      "1507448916.057173: Worker 0: training step 74 done (global step: 73)\n",
      "1507448916.060660: Worker 0: training step 75 done (global step: 74)\n",
      "1507448916.064004: Worker 0: training step 76 done (global step: 75)\n",
      "1507448916.067692: Worker 0: training step 77 done (global step: 76)\n",
      "1507448916.071405: Worker 0: training step 78 done (global step: 77)\n",
      "1507448916.074924: Worker 0: training step 79 done (global step: 78)\n",
      "1507448916.078275: Worker 0: training step 80 done (global step: 79)\n",
      "1507448916.082004: Worker 0: training step 81 done (global step: 80)\n",
      "1507448916.085714: Worker 0: training step 82 done (global step: 81)\n",
      "1507448916.089397: Worker 0: training step 83 done (global step: 82)\n",
      "1507448916.094752: Worker 0: training step 84 done (global step: 83)\n",
      "1507448916.100880: Worker 0: training step 85 done (global step: 84)\n",
      "1507448916.104796: Worker 0: training step 86 done (global step: 85)\n",
      "1507448916.108317: Worker 0: training step 87 done (global step: 86)\n",
      "1507448916.112037: Worker 0: training step 88 done (global step: 87)\n",
      "1507448916.116871: Worker 0: training step 89 done (global step: 88)\n",
      "1507448916.120548: Worker 0: training step 90 done (global step: 89)\n",
      "1507448916.123892: Worker 0: training step 91 done (global step: 90)\n",
      "1507448916.127496: Worker 0: training step 92 done (global step: 91)\n",
      "1507448916.132906: Worker 0: training step 93 done (global step: 92)\n",
      "1507448916.141652: Worker 0: training step 94 done (global step: 93)\n",
      "1507448916.148737: Worker 0: training step 95 done (global step: 94)\n",
      "1507448916.159378: Worker 0: training step 96 done (global step: 95)\n",
      "1507448916.164150: Worker 0: training step 97 done (global step: 96)\n",
      "1507448916.168054: Worker 0: training step 98 done (global step: 97)\n",
      "1507448916.172250: Worker 0: training step 99 done (global step: 98)\n",
      "1507448916.177079: Worker 0: training step 100 done (global step: 99)\n",
      "1507448916.180982: Worker 0: training step 101 done (global step: 100)\n",
      "1507448916.187323: Worker 0: training step 102 done (global step: 101)\n",
      "1507448916.192335: Worker 0: training step 103 done (global step: 102)\n",
      "1507448916.197163: Worker 0: training step 104 done (global step: 103)\n",
      "1507448916.201572: Worker 0: training step 105 done (global step: 104)\n",
      "1507448916.210862: Worker 0: training step 106 done (global step: 105)\n",
      "1507448916.217224: Worker 0: training step 107 done (global step: 106)\n",
      "1507448916.225345: Worker 0: training step 108 done (global step: 107)\n",
      "1507448916.233474: Worker 0: training step 109 done (global step: 108)\n",
      "1507448916.237523: Worker 0: training step 110 done (global step: 109)\n",
      "1507448916.240880: Worker 0: training step 111 done (global step: 110)\n",
      "1507448916.248972: Worker 0: training step 112 done (global step: 111)\n",
      "1507448916.253287: Worker 0: training step 113 done (global step: 112)\n",
      "1507448916.257193: Worker 0: training step 114 done (global step: 113)\n",
      "1507448916.260956: Worker 0: training step 115 done (global step: 114)\n",
      "1507448916.265038: Worker 0: training step 116 done (global step: 115)\n",
      "1507448916.268621: Worker 0: training step 117 done (global step: 116)\n",
      "1507448916.272155: Worker 0: training step 118 done (global step: 117)\n",
      "1507448916.276039: Worker 0: training step 119 done (global step: 118)\n",
      "1507448916.285753: Worker 0: training step 120 done (global step: 119)\n",
      "1507448916.294197: Worker 0: training step 121 done (global step: 120)\n",
      "1507448916.305788: Worker 0: training step 122 done (global step: 121)\n",
      "1507448916.310316: Worker 0: training step 123 done (global step: 122)\n",
      "1507448916.315868: Worker 0: training step 124 done (global step: 123)\n",
      "1507448916.321882: Worker 0: training step 125 done (global step: 124)\n",
      "1507448916.325401: Worker 0: training step 126 done (global step: 125)\n",
      "1507448916.333339: Worker 0: training step 127 done (global step: 126)\n",
      "1507448916.339995: Worker 0: training step 128 done (global step: 127)\n",
      "1507448916.345408: Worker 0: training step 129 done (global step: 128)\n",
      "1507448916.348859: Worker 0: training step 130 done (global step: 129)\n",
      "1507448916.352422: Worker 0: training step 131 done (global step: 130)\n",
      "1507448916.356108: Worker 0: training step 132 done (global step: 131)\n",
      "1507448916.372973: Worker 0: training step 133 done (global step: 132)\n",
      "1507448916.376385: Worker 0: training step 134 done (global step: 133)\n",
      "1507448916.379834: Worker 0: training step 135 done (global step: 134)\n",
      "1507448916.382959: Worker 0: training step 136 done (global step: 135)\n",
      "1507448916.388518: Worker 0: training step 137 done (global step: 136)\n",
      "1507448916.392063: Worker 0: training step 138 done (global step: 137)\n",
      "1507448916.397899: Worker 0: training step 139 done (global step: 138)\n",
      "1507448916.401386: Worker 0: training step 140 done (global step: 139)\n",
      "1507448916.404920: Worker 0: training step 141 done (global step: 140)\n",
      "1507448916.408553: Worker 0: training step 142 done (global step: 141)\n",
      "1507448916.411869: Worker 0: training step 143 done (global step: 142)\n",
      "1507448916.416770: Worker 0: training step 144 done (global step: 143)\n",
      "1507448916.427213: Worker 0: training step 145 done (global step: 144)\n",
      "1507448916.430635: Worker 0: training step 146 done (global step: 145)\n",
      "1507448916.434461: Worker 0: training step 147 done (global step: 146)\n",
      "1507448916.437934: Worker 0: training step 148 done (global step: 147)\n",
      "1507448916.442396: Worker 0: training step 149 done (global step: 148)\n",
      "1507448916.459053: Worker 0: training step 150 done (global step: 149)\n",
      "1507448916.462653: Worker 0: training step 151 done (global step: 150)\n",
      "1507448916.467338: Worker 0: training step 152 done (global step: 151)\n",
      "1507448916.474126: Worker 0: training step 153 done (global step: 152)\n",
      "1507448916.477463: Worker 0: training step 154 done (global step: 153)\n",
      "1507448916.481216: Worker 0: training step 155 done (global step: 154)\n",
      "1507448916.484666: Worker 0: training step 156 done (global step: 155)\n",
      "1507448916.487987: Worker 0: training step 157 done (global step: 156)\n",
      "1507448916.491248: Worker 0: training step 158 done (global step: 157)\n",
      "1507448916.495301: Worker 0: training step 159 done (global step: 158)\n",
      "1507448916.498584: Worker 0: training step 160 done (global step: 159)\n",
      "1507448916.502055: Worker 0: training step 161 done (global step: 160)\n",
      "1507448916.505401: Worker 0: training step 162 done (global step: 161)\n",
      "1507448916.508649: Worker 0: training step 163 done (global step: 162)\n",
      "1507448916.524429: Worker 0: training step 164 done (global step: 163)\n",
      "1507448916.534096: Worker 0: training step 165 done (global step: 164)\n",
      "1507448916.541416: Worker 0: training step 166 done (global step: 165)\n",
      "1507448916.548993: Worker 0: training step 167 done (global step: 166)\n",
      "1507448916.553247: Worker 0: training step 168 done (global step: 167)\n",
      "1507448916.561247: Worker 0: training step 169 done (global step: 168)\n",
      "1507448916.564871: Worker 0: training step 170 done (global step: 169)\n",
      "1507448916.568339: Worker 0: training step 171 done (global step: 170)\n",
      "1507448916.571815: Worker 0: training step 172 done (global step: 171)\n",
      "1507448916.579373: Worker 0: training step 173 done (global step: 172)\n",
      "1507448916.582900: Worker 0: training step 174 done (global step: 173)\n",
      "1507448916.586336: Worker 0: training step 175 done (global step: 174)\n",
      "1507448916.594364: Worker 0: training step 176 done (global step: 175)\n",
      "1507448916.597966: Worker 0: training step 177 done (global step: 176)\n",
      "1507448916.603443: Worker 0: training step 178 done (global step: 177)\n",
      "1507448916.607083: Worker 0: training step 179 done (global step: 178)\n",
      "1507448916.611664: Worker 0: training step 180 done (global step: 179)\n",
      "1507448916.615234: Worker 0: training step 181 done (global step: 180)\n",
      "1507448916.618836: Worker 0: training step 182 done (global step: 181)\n",
      "1507448916.631459: Worker 0: training step 183 done (global step: 182)\n",
      "1507448916.637253: Worker 0: training step 184 done (global step: 183)\n",
      "1507448916.648877: Worker 0: training step 185 done (global step: 184)\n",
      "1507448916.652780: Worker 0: training step 186 done (global step: 185)\n",
      "1507448916.656499: Worker 0: training step 187 done (global step: 186)\n",
      "1507448916.660247: Worker 0: training step 188 done (global step: 187)\n",
      "1507448916.663760: Worker 0: training step 189 done (global step: 188)\n",
      "1507448916.672287: Worker 0: training step 190 done (global step: 189)\n",
      "1507448916.679135: Worker 0: training step 191 done (global step: 190)\n",
      "1507448916.704302: Worker 0: training step 192 done (global step: 191)\n",
      "1507448916.708142: Worker 0: training step 193 done (global step: 192)\n",
      "1507448916.711678: Worker 0: training step 194 done (global step: 193)\n",
      "1507448916.715775: Worker 0: training step 195 done (global step: 194)\n",
      "1507448916.720344: Worker 0: training step 196 done (global step: 195)\n",
      "1507448916.735621: Worker 0: training step 197 done (global step: 196)\n",
      "1507448916.739706: Worker 0: training step 198 done (global step: 197)\n",
      "1507448916.749459: Worker 0: training step 199 done (global step: 198)\n",
      "1507448916.753351: Worker 0: training step 200 done (global step: 199)\n",
      "1507448916.756809: Worker 0: training step 201 done (global step: 200)\n",
      "1507448916.760255: Worker 0: training step 202 done (global step: 201)\n",
      "1507448916.763855: Worker 0: training step 203 done (global step: 202)\n",
      "1507448916.769651: Worker 0: training step 204 done (global step: 203)\n",
      "1507448916.777303: Worker 0: training step 205 done (global step: 204)\n",
      "1507448916.780813: Worker 0: training step 206 done (global step: 205)\n",
      "1507448916.784238: Worker 0: training step 207 done (global step: 206)\n",
      "1507448916.787914: Worker 0: training step 208 done (global step: 207)\n",
      "1507448916.791455: Worker 0: training step 209 done (global step: 208)\n",
      "1507448916.794854: Worker 0: training step 210 done (global step: 209)\n",
      "1507448916.800043: Worker 0: training step 211 done (global step: 210)\n",
      "1507448916.804221: Worker 0: training step 212 done (global step: 211)\n",
      "1507448916.813630: Worker 0: training step 213 done (global step: 212)\n",
      "1507448916.818628: Worker 0: training step 214 done (global step: 213)\n",
      "1507448916.822274: Worker 0: training step 215 done (global step: 214)\n",
      "1507448916.826111: Worker 0: training step 216 done (global step: 215)\n",
      "1507448916.830214: Worker 0: training step 217 done (global step: 216)\n",
      "1507448916.835281: Worker 0: training step 218 done (global step: 217)\n",
      "1507448916.844141: Worker 0: training step 219 done (global step: 218)\n",
      "1507448916.853452: Worker 0: training step 220 done (global step: 219)\n",
      "1507448916.860330: Worker 0: training step 221 done (global step: 220)\n",
      "1507448916.864133: Worker 0: training step 222 done (global step: 221)\n",
      "1507448916.867591: Worker 0: training step 223 done (global step: 222)\n",
      "1507448916.878982: Worker 0: training step 224 done (global step: 223)\n",
      "1507448916.888396: Worker 0: training step 225 done (global step: 224)\n",
      "1507448916.901095: Worker 0: training step 226 done (global step: 225)\n",
      "1507448916.905164: Worker 0: training step 227 done (global step: 226)\n",
      "1507448916.924855: Worker 0: training step 228 done (global step: 227)\n",
      "1507448916.933007: Worker 0: training step 229 done (global step: 228)\n",
      "1507448916.936900: Worker 0: training step 230 done (global step: 229)\n",
      "1507448916.947272: Worker 0: training step 231 done (global step: 230)\n",
      "1507448916.953888: Worker 0: training step 232 done (global step: 231)\n",
      "1507448916.959018: Worker 0: training step 233 done (global step: 232)\n",
      "1507448916.966870: Worker 0: training step 234 done (global step: 233)\n",
      "1507448916.970427: Worker 0: training step 235 done (global step: 234)\n",
      "1507448916.974043: Worker 0: training step 236 done (global step: 235)\n",
      "1507448916.977899: Worker 0: training step 237 done (global step: 236)\n",
      "1507448916.984851: Worker 0: training step 238 done (global step: 238)\n",
      "1507448916.988692: Worker 0: training step 239 done (global step: 239)\n",
      "1507448916.995974: Worker 0: training step 240 done (global step: 241)\n",
      "1507448917.000504: Worker 0: training step 241 done (global step: 242)\n",
      "1507448917.006671: Worker 0: training step 242 done (global step: 243)\n",
      "1507448917.015370: Worker 0: training step 243 done (global step: 245)\n",
      "1507448917.027121: Worker 0: training step 244 done (global step: 247)\n",
      "1507448917.046678: Worker 0: training step 245 done (global step: 249)\n",
      "1507448917.058898: Worker 0: training step 246 done (global step: 252)\n",
      "1507448917.065726: Worker 0: training step 247 done (global step: 254)\n",
      "1507448917.090952: Worker 0: training step 248 done (global step: 256)\n",
      "1507448917.103037: Worker 0: training step 249 done (global step: 259)\n",
      "1507448917.110825: Worker 0: training step 250 done (global step: 262)\n",
      "1507448917.118380: Worker 0: training step 251 done (global step: 264)\n",
      "1507448917.123843: Worker 0: training step 252 done (global step: 266)\n",
      "1507448917.135204: Worker 0: training step 253 done (global step: 268)\n",
      "1507448917.144491: Worker 0: training step 254 done (global step: 271)\n",
      "1507448917.154748: Worker 0: training step 255 done (global step: 273)\n",
      "1507448917.160379: Worker 0: training step 256 done (global step: 276)\n",
      "1507448917.165008: Worker 0: training step 257 done (global step: 278)\n",
      "1507448917.174902: Worker 0: training step 258 done (global step: 280)\n",
      "1507448917.181758: Worker 0: training step 259 done (global step: 283)\n",
      "1507448917.186201: Worker 0: training step 260 done (global step: 285)\n",
      "1507448917.190369: Worker 0: training step 261 done (global step: 287)\n",
      "1507448917.194545: Worker 0: training step 262 done (global step: 288)\n",
      "1507448917.200884: Worker 0: training step 263 done (global step: 290)\n",
      "1507448917.211221: Worker 0: training step 264 done (global step: 292)\n",
      "1507448917.216941: Worker 0: training step 265 done (global step: 294)\n",
      "1507448917.229751: Worker 0: training step 266 done (global step: 296)\n",
      "1507448917.235165: Worker 0: training step 267 done (global step: 298)\n",
      "1507448917.248208: Worker 0: training step 268 done (global step: 299)\n",
      "1507448917.259225: Worker 0: training step 269 done (global step: 303)\n",
      "1507448917.273739: Worker 0: training step 270 done (global step: 306)\n",
      "1507448917.279126: Worker 0: training step 271 done (global step: 308)\n",
      "1507448917.284551: Worker 0: training step 272 done (global step: 310)\n",
      "1507448917.297288: Worker 0: training step 273 done (global step: 312)\n",
      "1507448917.304441: Worker 0: training step 274 done (global step: 315)\n",
      "1507448917.314687: Worker 0: training step 275 done (global step: 317)\n",
      "1507448917.319786: Worker 0: training step 276 done (global step: 319)\n",
      "1507448917.334174: Worker 0: training step 277 done (global step: 321)\n",
      "1507448917.345304: Worker 0: training step 278 done (global step: 323)\n",
      "1507448917.359633: Worker 0: training step 279 done (global step: 326)\n",
      "1507448917.368514: Worker 0: training step 280 done (global step: 328)\n",
      "1507448917.378590: Worker 0: training step 281 done (global step: 331)\n",
      "1507448917.384573: Worker 0: training step 282 done (global step: 333)\n",
      "1507448917.388877: Worker 0: training step 283 done (global step: 335)\n",
      "1507448917.392962: Worker 0: training step 284 done (global step: 337)\n",
      "1507448917.397177: Worker 0: training step 285 done (global step: 339)\n",
      "1507448917.400901: Worker 0: training step 286 done (global step: 340)\n",
      "1507448917.404526: Worker 0: training step 287 done (global step: 342)\n",
      "1507448917.408351: Worker 0: training step 288 done (global step: 344)\n",
      "1507448917.412562: Worker 0: training step 289 done (global step: 346)\n",
      "1507448917.417175: Worker 0: training step 290 done (global step: 348)\n",
      "1507448917.421050: Worker 0: training step 291 done (global step: 349)\n",
      "1507448917.425218: Worker 0: training step 292 done (global step: 351)\n",
      "1507448917.428921: Worker 0: training step 293 done (global step: 353)\n",
      "1507448917.433194: Worker 0: training step 294 done (global step: 354)\n",
      "1507448917.437179: Worker 0: training step 295 done (global step: 356)\n",
      "1507448917.440715: Worker 0: training step 296 done (global step: 358)\n",
      "1507448917.444576: Worker 0: training step 297 done (global step: 360)\n",
      "1507448917.448363: Worker 0: training step 298 done (global step: 362)\n",
      "1507448917.452889: Worker 0: training step 299 done (global step: 363)\n",
      "1507448917.457297: Worker 0: training step 300 done (global step: 365)\n",
      "1507448917.461140: Worker 0: training step 301 done (global step: 367)\n",
      "1507448917.465343: Worker 0: training step 302 done (global step: 369)\n",
      "1507448917.469537: Worker 0: training step 303 done (global step: 371)\n",
      "1507448917.473631: Worker 0: training step 304 done (global step: 373)\n",
      "1507448917.478535: Worker 0: training step 305 done (global step: 374)\n",
      "1507448917.482861: Worker 0: training step 306 done (global step: 376)\n",
      "1507448917.486699: Worker 0: training step 307 done (global step: 378)\n",
      "1507448917.490874: Worker 0: training step 308 done (global step: 380)\n",
      "1507448917.494706: Worker 0: training step 309 done (global step: 382)\n",
      "1507448917.498769: Worker 0: training step 310 done (global step: 383)\n",
      "1507448917.502844: Worker 0: training step 311 done (global step: 385)\n",
      "1507448917.506726: Worker 0: training step 312 done (global step: 387)\n",
      "1507448917.510786: Worker 0: training step 313 done (global step: 389)\n",
      "1507448917.515404: Worker 0: training step 314 done (global step: 390)\n",
      "1507448917.522953: Worker 0: training step 315 done (global step: 392)\n",
      "1507448917.526749: Worker 0: training step 316 done (global step: 395)\n",
      "1507448917.530708: Worker 0: training step 317 done (global step: 397)\n",
      "1507448917.535090: Worker 0: training step 318 done (global step: 398)\n",
      "1507448917.539976: Worker 0: training step 319 done (global step: 400)\n",
      "1507448917.544360: Worker 0: training step 320 done (global step: 402)\n",
      "1507448917.547666: Worker 0: training step 321 done (global step: 404)\n",
      "1507448917.551582: Worker 0: training step 322 done (global step: 405)\n",
      "1507448917.555877: Worker 0: training step 323 done (global step: 407)\n",
      "1507448917.560292: Worker 0: training step 324 done (global step: 409)\n",
      "1507448917.564669: Worker 0: training step 325 done (global step: 411)\n",
      "1507448917.568402: Worker 0: training step 326 done (global step: 413)\n",
      "1507448917.572415: Worker 0: training step 327 done (global step: 415)\n",
      "1507448917.576544: Worker 0: training step 328 done (global step: 416)\n",
      "1507448917.580486: Worker 0: training step 329 done (global step: 418)\n",
      "1507448917.584239: Worker 0: training step 330 done (global step: 420)\n",
      "1507448917.588087: Worker 0: training step 331 done (global step: 422)\n",
      "1507448917.592208: Worker 0: training step 332 done (global step: 423)\n",
      "1507448917.596100: Worker 0: training step 333 done (global step: 425)\n",
      "1507448917.600069: Worker 0: training step 334 done (global step: 427)\n",
      "1507448917.604516: Worker 0: training step 335 done (global step: 429)\n",
      "1507448917.608719: Worker 0: training step 336 done (global step: 431)\n",
      "1507448917.612467: Worker 0: training step 337 done (global step: 433)\n",
      "1507448917.615958: Worker 0: training step 338 done (global step: 434)\n",
      "1507448917.619765: Worker 0: training step 339 done (global step: 436)\n",
      "1507448917.623586: Worker 0: training step 340 done (global step: 437)\n",
      "1507448917.627399: Worker 0: training step 341 done (global step: 439)\n",
      "1507448917.631788: Worker 0: training step 342 done (global step: 441)\n",
      "1507448917.635283: Worker 0: training step 343 done (global step: 443)\n",
      "1507448917.639331: Worker 0: training step 344 done (global step: 444)\n",
      "1507448917.643076: Worker 0: training step 345 done (global step: 446)\n",
      "1507448917.647331: Worker 0: training step 346 done (global step: 448)\n",
      "1507448917.651242: Worker 0: training step 347 done (global step: 450)\n",
      "1507448917.655177: Worker 0: training step 348 done (global step: 451)\n",
      "1507448917.659006: Worker 0: training step 349 done (global step: 453)\n",
      "1507448917.662628: Worker 0: training step 350 done (global step: 455)\n",
      "1507448917.667392: Worker 0: training step 351 done (global step: 457)\n",
      "1507448917.671567: Worker 0: training step 352 done (global step: 458)\n",
      "1507448917.675268: Worker 0: training step 353 done (global step: 460)\n",
      "1507448917.679139: Worker 0: training step 354 done (global step: 461)\n",
      "1507448917.694603: Worker 0: training step 355 done (global step: 463)\n",
      "1507448917.698552: Worker 0: training step 356 done (global step: 465)\n",
      "1507448917.703474: Worker 0: training step 357 done (global step: 466)\n",
      "1507448917.707167: Worker 0: training step 358 done (global step: 468)\n",
      "1507448917.711765: Worker 0: training step 359 done (global step: 470)\n",
      "1507448917.715029: Worker 0: training step 360 done (global step: 472)\n",
      "1507448917.718782: Worker 0: training step 361 done (global step: 473)\n",
      "1507448917.722446: Worker 0: training step 362 done (global step: 475)\n",
      "1507448917.725948: Worker 0: training step 363 done (global step: 477)\n",
      "1507448917.730627: Worker 0: training step 364 done (global step: 478)\n",
      "1507448917.733909: Worker 0: training step 365 done (global step: 480)\n",
      "1507448917.737396: Worker 0: training step 366 done (global step: 482)\n",
      "1507448917.741353: Worker 0: training step 367 done (global step: 483)\n",
      "1507448917.745502: Worker 0: training step 368 done (global step: 485)\n",
      "1507448917.748663: Worker 0: training step 369 done (global step: 487)\n",
      "1507448917.752442: Worker 0: training step 370 done (global step: 488)\n",
      "1507448917.756103: Worker 0: training step 371 done (global step: 490)\n",
      "1507448917.759904: Worker 0: training step 372 done (global step: 492)\n",
      "1507448917.763389: Worker 0: training step 373 done (global step: 494)\n",
      "1507448917.767199: Worker 0: training step 374 done (global step: 495)\n",
      "1507448917.771284: Worker 0: training step 375 done (global step: 497)\n",
      "1507448917.784630: Worker 0: training step 376 done (global step: 499)\n",
      "1507448917.788744: Worker 0: training step 377 done (global step: 500)\n",
      "1507448917.792960: Worker 0: training step 378 done (global step: 502)\n",
      "1507448917.796353: Worker 0: training step 379 done (global step: 504)\n",
      "1507448917.800056: Worker 0: training step 380 done (global step: 505)\n",
      "1507448917.803926: Worker 0: training step 381 done (global step: 507)\n",
      "1507448917.807601: Worker 0: training step 382 done (global step: 509)\n",
      "1507448917.811899: Worker 0: training step 383 done (global step: 510)\n",
      "1507448917.815073: Worker 0: training step 384 done (global step: 512)\n",
      "1507448917.819215: Worker 0: training step 385 done (global step: 513)\n",
      "1507448917.823198: Worker 0: training step 386 done (global step: 515)\n",
      "1507448917.826932: Worker 0: training step 387 done (global step: 517)\n",
      "1507448917.831101: Worker 0: training step 388 done (global step: 518)\n",
      "1507448917.834613: Worker 0: training step 389 done (global step: 520)\n",
      "1507448917.838979: Worker 0: training step 390 done (global step: 522)\n",
      "1507448917.842627: Worker 0: training step 391 done (global step: 524)\n",
      "1507448917.846360: Worker 0: training step 392 done (global step: 525)\n",
      "1507448917.850737: Worker 0: training step 393 done (global step: 527)\n",
      "1507448917.854925: Worker 0: training step 394 done (global step: 529)\n",
      "1507448917.858621: Worker 0: training step 395 done (global step: 531)\n",
      "1507448917.862445: Worker 0: training step 396 done (global step: 532)\n",
      "1507448917.865983: Worker 0: training step 397 done (global step: 534)\n",
      "1507448917.869742: Worker 0: training step 398 done (global step: 536)\n",
      "1507448917.873838: Worker 0: training step 399 done (global step: 538)\n",
      "1507448917.877864: Worker 0: training step 400 done (global step: 539)\n",
      "1507448917.881497: Worker 0: training step 401 done (global step: 541)\n",
      "1507448917.893206: Worker 0: training step 402 done (global step: 543)\n",
      "1507448917.897695: Worker 0: training step 403 done (global step: 545)\n",
      "1507448917.902947: Worker 0: training step 404 done (global step: 547)\n",
      "1507448917.907679: Worker 0: training step 405 done (global step: 548)\n",
      "1507448917.911694: Worker 0: training step 406 done (global step: 550)\n",
      "1507448917.916251: Worker 0: training step 407 done (global step: 552)\n",
      "1507448917.921349: Worker 0: training step 408 done (global step: 553)\n",
      "1507448917.926127: Worker 0: training step 409 done (global step: 555)\n",
      "1507448917.930355: Worker 0: training step 410 done (global step: 557)\n",
      "1507448917.934723: Worker 0: training step 411 done (global step: 559)\n",
      "1507448917.938973: Worker 0: training step 412 done (global step: 560)\n",
      "1507448917.943072: Worker 0: training step 413 done (global step: 562)\n",
      "1507448917.947281: Worker 0: training step 414 done (global step: 564)\n",
      "1507448917.953066: Worker 0: training step 415 done (global step: 566)\n",
      "1507448917.961436: Worker 0: training step 416 done (global step: 568)\n",
      "1507448917.965266: Worker 0: training step 417 done (global step: 570)\n",
      "1507448917.970030: Worker 0: training step 418 done (global step: 571)\n",
      "1507448917.974745: Worker 0: training step 419 done (global step: 573)\n",
      "1507448917.979130: Worker 0: training step 420 done (global step: 575)\n",
      "1507448917.983296: Worker 0: training step 421 done (global step: 577)\n",
      "1507448917.987632: Worker 0: training step 422 done (global step: 579)\n",
      "1507448917.992181: Worker 0: training step 423 done (global step: 581)\n",
      "1507448917.996623: Worker 0: training step 424 done (global step: 583)\n",
      "1507448918.001118: Worker 0: training step 425 done (global step: 584)\n",
      "1507448918.005422: Worker 0: training step 426 done (global step: 586)\n",
      "1507448918.010197: Worker 0: training step 427 done (global step: 588)\n",
      "1507448918.014729: Worker 0: training step 428 done (global step: 590)\n",
      "1507448918.019769: Worker 0: training step 429 done (global step: 592)\n",
      "1507448918.023820: Worker 0: training step 430 done (global step: 594)\n",
      "1507448918.029658: Worker 0: training step 431 done (global step: 596)\n",
      "1507448918.033903: Worker 0: training step 432 done (global step: 598)\n",
      "1507448918.037529: Worker 0: training step 433 done (global step: 599)\n",
      "1507448918.041520: Worker 0: training step 434 done (global step: 601)\n",
      "1507448918.045986: Worker 0: training step 435 done (global step: 603)\n",
      "1507448918.049999: Worker 0: training step 436 done (global step: 604)\n",
      "1507448918.054473: Worker 0: training step 437 done (global step: 606)\n",
      "1507448918.059239: Worker 0: training step 438 done (global step: 608)\n",
      "1507448918.062922: Worker 0: training step 439 done (global step: 610)\n",
      "1507448918.067282: Worker 0: training step 440 done (global step: 611)\n",
      "1507448918.071330: Worker 0: training step 441 done (global step: 613)\n",
      "1507448918.075029: Worker 0: training step 442 done (global step: 615)\n",
      "1507448918.079140: Worker 0: training step 443 done (global step: 617)\n",
      "1507448918.083321: Worker 0: training step 444 done (global step: 619)\n",
      "1507448918.088410: Worker 0: training step 445 done (global step: 620)\n",
      "1507448918.092702: Worker 0: training step 446 done (global step: 622)\n",
      "1507448918.096274: Worker 0: training step 447 done (global step: 624)\n",
      "1507448918.099795: Worker 0: training step 448 done (global step: 626)\n",
      "1507448918.103808: Worker 0: training step 449 done (global step: 628)\n",
      "1507448918.107788: Worker 0: training step 450 done (global step: 629)\n",
      "1507448918.111866: Worker 0: training step 451 done (global step: 631)\n",
      "1507448918.115485: Worker 0: training step 452 done (global step: 633)\n",
      "1507448918.120395: Worker 0: training step 453 done (global step: 634)\n",
      "1507448918.124468: Worker 0: training step 454 done (global step: 636)\n",
      "1507448918.128005: Worker 0: training step 455 done (global step: 638)\n",
      "1507448918.131988: Worker 0: training step 456 done (global step: 640)\n",
      "1507448918.135800: Worker 0: training step 457 done (global step: 642)\n",
      "1507448918.148835: Worker 0: training step 458 done (global step: 645)\n",
      "1507448918.152389: Worker 0: training step 459 done (global step: 647)\n",
      "1507448918.156105: Worker 0: training step 460 done (global step: 649)\n",
      "1507448918.159642: Worker 0: training step 461 done (global step: 650)\n",
      "1507448918.162841: Worker 0: training step 462 done (global step: 652)\n",
      "1507448918.166194: Worker 0: training step 463 done (global step: 653)\n",
      "1507448918.169357: Worker 0: training step 464 done (global step: 654)\n",
      "1507448918.172982: Worker 0: training step 465 done (global step: 655)\n",
      "1507448918.176951: Worker 0: training step 466 done (global step: 657)\n",
      "1507448918.181388: Worker 0: training step 467 done (global step: 659)\n",
      "1507448918.185343: Worker 0: training step 468 done (global step: 661)\n",
      "1507448918.190042: Worker 0: training step 469 done (global step: 662)\n",
      "1507448918.194497: Worker 0: training step 470 done (global step: 664)\n",
      "1507448918.198176: Worker 0: training step 471 done (global step: 666)\n",
      "1507448918.201969: Worker 0: training step 472 done (global step: 668)\n",
      "1507448918.206017: Worker 0: training step 473 done (global step: 670)\n",
      "1507448918.209553: Worker 0: training step 474 done (global step: 671)\n",
      "1507448918.213288: Worker 0: training step 475 done (global step: 673)\n",
      "1507448918.217165: Worker 0: training step 476 done (global step: 675)\n",
      "1507448918.220995: Worker 0: training step 477 done (global step: 677)\n",
      "1507448918.225047: Worker 0: training step 478 done (global step: 678)\n",
      "1507448918.229208: Worker 0: training step 479 done (global step: 680)\n",
      "1507448918.232886: Worker 0: training step 480 done (global step: 682)\n",
      "1507448918.236210: Worker 0: training step 481 done (global step: 684)\n",
      "1507448918.240595: Worker 0: training step 482 done (global step: 685)\n",
      "1507448918.245338: Worker 0: training step 483 done (global step: 687)\n",
      "1507448918.250042: Worker 0: training step 484 done (global step: 689)\n",
      "1507448918.253534: Worker 0: training step 485 done (global step: 691)\n",
      "1507448918.257241: Worker 0: training step 486 done (global step: 693)\n",
      "1507448918.260781: Worker 0: training step 487 done (global step: 695)\n",
      "1507448918.264563: Worker 0: training step 488 done (global step: 696)\n",
      "1507448918.268402: Worker 0: training step 489 done (global step: 698)\n",
      "1507448918.272248: Worker 0: training step 490 done (global step: 700)\n",
      "1507448918.276546: Worker 0: training step 491 done (global step: 701)\n",
      "1507448918.279895: Worker 0: training step 492 done (global step: 703)\n",
      "1507448918.283793: Worker 0: training step 493 done (global step: 705)\n",
      "1507448918.287134: Worker 0: training step 494 done (global step: 707)\n",
      "1507448918.291078: Worker 0: training step 495 done (global step: 709)\n",
      "1507448918.295195: Worker 0: training step 496 done (global step: 710)\n",
      "1507448918.298705: Worker 0: training step 497 done (global step: 712)\n",
      "1507448918.303929: Worker 0: training step 498 done (global step: 713)\n",
      "1507448918.308280: Worker 0: training step 499 done (global step: 715)\n",
      "1507448918.311859: Worker 0: training step 500 done (global step: 717)\n",
      "1507448918.315807: Worker 0: training step 501 done (global step: 719)\n",
      "1507448918.320599: Worker 0: training step 502 done (global step: 721)\n",
      "1507448918.324543: Worker 0: training step 503 done (global step: 723)\n",
      "1507448918.329798: Worker 0: training step 504 done (global step: 724)\n",
      "1507448918.333665: Worker 0: training step 505 done (global step: 726)\n",
      "1507448918.338723: Worker 0: training step 506 done (global step: 728)\n",
      "1507448918.342313: Worker 0: training step 507 done (global step: 730)\n",
      "1507448918.346745: Worker 0: training step 508 done (global step: 732)\n",
      "1507448918.351048: Worker 0: training step 509 done (global step: 733)\n",
      "1507448918.354688: Worker 0: training step 510 done (global step: 735)\n",
      "1507448918.358507: Worker 0: training step 511 done (global step: 737)\n",
      "1507448918.362273: Worker 0: training step 512 done (global step: 739)\n",
      "1507448918.365455: Worker 0: training step 513 done (global step: 740)\n",
      "1507448918.368754: Worker 0: training step 514 done (global step: 741)\n",
      "1507448918.372269: Worker 0: training step 515 done (global step: 742)\n",
      "1507448918.376077: Worker 0: training step 516 done (global step: 744)\n",
      "1507448918.380043: Worker 0: training step 517 done (global step: 746)\n",
      "1507448918.384005: Worker 0: training step 518 done (global step: 747)\n",
      "1507448918.387970: Worker 0: training step 519 done (global step: 749)\n",
      "1507448918.392035: Worker 0: training step 520 done (global step: 751)\n",
      "1507448918.395520: Worker 0: training step 521 done (global step: 753)\n",
      "1507448918.400511: Worker 0: training step 522 done (global step: 754)\n",
      "1507448918.404605: Worker 0: training step 523 done (global step: 756)\n",
      "1507448918.408480: Worker 0: training step 524 done (global step: 758)\n",
      "1507448918.412717: Worker 0: training step 525 done (global step: 760)\n",
      "1507448918.416986: Worker 0: training step 526 done (global step: 762)\n",
      "1507448918.420679: Worker 0: training step 527 done (global step: 764)\n",
      "1507448918.425127: Worker 0: training step 528 done (global step: 765)\n",
      "1507448918.429489: Worker 0: training step 529 done (global step: 767)\n",
      "1507448918.433238: Worker 0: training step 530 done (global step: 769)\n",
      "1507448918.437262: Worker 0: training step 531 done (global step: 771)\n",
      "1507448918.440924: Worker 0: training step 532 done (global step: 773)\n",
      "1507448918.445207: Worker 0: training step 533 done (global step: 774)\n",
      "1507448918.448819: Worker 0: training step 534 done (global step: 776)\n",
      "1507448918.452368: Worker 0: training step 535 done (global step: 778)\n",
      "1507448918.456582: Worker 0: training step 536 done (global step: 779)\n",
      "1507448918.460475: Worker 0: training step 537 done (global step: 781)\n",
      "1507448918.463711: Worker 0: training step 538 done (global step: 783)\n",
      "1507448918.467717: Worker 0: training step 539 done (global step: 784)\n",
      "1507448918.471152: Worker 0: training step 540 done (global step: 786)\n",
      "1507448918.475471: Worker 0: training step 541 done (global step: 788)\n",
      "1507448918.479148: Worker 0: training step 542 done (global step: 790)\n",
      "1507448918.483538: Worker 0: training step 543 done (global step: 792)\n",
      "1507448918.487981: Worker 0: training step 544 done (global step: 793)\n",
      "1507448918.491840: Worker 0: training step 545 done (global step: 795)\n",
      "1507448918.495525: Worker 0: training step 546 done (global step: 797)\n",
      "1507448918.499070: Worker 0: training step 547 done (global step: 799)\n",
      "1507448918.503107: Worker 0: training step 548 done (global step: 800)\n",
      "1507448918.506770: Worker 0: training step 549 done (global step: 802)\n",
      "1507448918.510488: Worker 0: training step 550 done (global step: 804)\n",
      "1507448918.588835: Worker 0: training step 551 done (global step: 818)\n",
      "1507448918.593093: Worker 0: training step 552 done (global step: 820)\n",
      "1507448918.596876: Worker 0: training step 553 done (global step: 822)\n",
      "1507448918.601250: Worker 0: training step 554 done (global step: 823)\n",
      "1507448918.605116: Worker 0: training step 555 done (global step: 825)\n",
      "1507448918.609714: Worker 0: training step 556 done (global step: 827)\n",
      "1507448918.614166: Worker 0: training step 557 done (global step: 829)\n",
      "1507448918.617585: Worker 0: training step 558 done (global step: 830)\n",
      "1507448918.624156: Worker 0: training step 559 done (global step: 831)\n",
      "1507448918.628703: Worker 0: training step 560 done (global step: 833)\n",
      "1507448918.632357: Worker 0: training step 561 done (global step: 835)\n",
      "1507448918.636762: Worker 0: training step 562 done (global step: 837)\n",
      "1507448918.640630: Worker 0: training step 563 done (global step: 839)\n",
      "1507448918.644301: Worker 0: training step 564 done (global step: 840)\n",
      "1507448918.648685: Worker 0: training step 565 done (global step: 842)\n",
      "1507448918.662152: Worker 0: training step 566 done (global step: 844)\n",
      "1507448918.665541: Worker 0: training step 567 done (global step: 846)\n",
      "1507448918.669502: Worker 0: training step 568 done (global step: 847)\n",
      "1507448918.673330: Worker 0: training step 569 done (global step: 849)\n",
      "1507448918.676825: Worker 0: training step 570 done (global step: 851)\n",
      "1507448918.680944: Worker 0: training step 571 done (global step: 852)\n",
      "1507448918.687061: Worker 0: training step 572 done (global step: 854)\n",
      "1507448918.690782: Worker 0: training step 573 done (global step: 855)\n",
      "1507448918.694158: Worker 0: training step 574 done (global step: 857)\n",
      "1507448918.697948: Worker 0: training step 575 done (global step: 858)\n",
      "1507448918.701322: Worker 0: training step 576 done (global step: 860)\n",
      "1507448918.705191: Worker 0: training step 577 done (global step: 862)\n",
      "1507448918.709030: Worker 0: training step 578 done (global step: 863)\n",
      "1507448918.713533: Worker 0: training step 579 done (global step: 865)\n",
      "1507448918.717007: Worker 0: training step 580 done (global step: 867)\n",
      "1507448918.720960: Worker 0: training step 581 done (global step: 869)\n",
      "1507448918.724606: Worker 0: training step 582 done (global step: 870)\n",
      "1507448918.728886: Worker 0: training step 583 done (global step: 871)\n",
      "1507448918.732893: Worker 0: training step 584 done (global step: 873)\n",
      "1507448918.737380: Worker 0: training step 585 done (global step: 874)\n",
      "1507448918.741427: Worker 0: training step 586 done (global step: 876)\n",
      "1507448918.745350: Worker 0: training step 587 done (global step: 878)\n",
      "1507448918.749510: Worker 0: training step 588 done (global step: 880)\n",
      "1507448918.753071: Worker 0: training step 589 done (global step: 881)\n",
      "1507448918.757001: Worker 0: training step 590 done (global step: 882)\n",
      "1507448918.760934: Worker 0: training step 591 done (global step: 884)\n",
      "1507448918.765764: Worker 0: training step 592 done (global step: 886)\n",
      "1507448918.769501: Worker 0: training step 593 done (global step: 888)\n",
      "1507448918.773306: Worker 0: training step 594 done (global step: 889)\n",
      "1507448918.777692: Worker 0: training step 595 done (global step: 891)\n",
      "1507448918.781594: Worker 0: training step 596 done (global step: 893)\n",
      "1507448918.785267: Worker 0: training step 597 done (global step: 895)\n",
      "1507448918.789295: Worker 0: training step 598 done (global step: 896)\n",
      "1507448918.793266: Worker 0: training step 599 done (global step: 898)\n",
      "1507448918.797311: Worker 0: training step 600 done (global step: 900)\n",
      "1507448918.801124: Worker 0: training step 601 done (global step: 902)\n",
      "1507448918.805592: Worker 0: training step 602 done (global step: 903)\n",
      "1507448918.809288: Worker 0: training step 603 done (global step: 905)\n",
      "1507448918.812795: Worker 0: training step 604 done (global step: 907)\n",
      "1507448918.816328: Worker 0: training step 605 done (global step: 908)\n",
      "1507448918.820359: Worker 0: training step 606 done (global step: 909)\n",
      "1507448918.824038: Worker 0: training step 607 done (global step: 911)\n",
      "1507448918.827485: Worker 0: training step 608 done (global step: 912)\n",
      "1507448918.831529: Worker 0: training step 609 done (global step: 913)\n",
      "1507448918.835245: Worker 0: training step 610 done (global step: 915)\n",
      "1507448918.839543: Worker 0: training step 611 done (global step: 916)\n",
      "1507448918.843607: Worker 0: training step 612 done (global step: 918)\n",
      "1507448918.847284: Worker 0: training step 613 done (global step: 920)\n",
      "1507448918.851452: Worker 0: training step 614 done (global step: 922)\n",
      "1507448918.855866: Worker 0: training step 615 done (global step: 923)\n",
      "1507448918.859756: Worker 0: training step 616 done (global step: 925)\n",
      "1507448918.863472: Worker 0: training step 617 done (global step: 927)\n",
      "1507448918.867313: Worker 0: training step 618 done (global step: 929)\n",
      "1507448918.871319: Worker 0: training step 619 done (global step: 930)\n",
      "1507448918.874930: Worker 0: training step 620 done (global step: 932)\n",
      "1507448918.878695: Worker 0: training step 621 done (global step: 933)\n",
      "1507448918.882897: Worker 0: training step 622 done (global step: 935)\n",
      "1507448918.887239: Worker 0: training step 623 done (global step: 937)\n",
      "1507448918.890559: Worker 0: training step 624 done (global step: 938)\n",
      "1507448918.894641: Worker 0: training step 625 done (global step: 939)\n",
      "1507448918.898182: Worker 0: training step 626 done (global step: 941)\n",
      "1507448918.902055: Worker 0: training step 627 done (global step: 943)\n",
      "1507448918.906004: Worker 0: training step 628 done (global step: 944)\n",
      "1507448918.909593: Worker 0: training step 629 done (global step: 946)\n",
      "1507448918.913101: Worker 0: training step 630 done (global step: 948)\n",
      "1507448918.917006: Worker 0: training step 631 done (global step: 949)\n",
      "1507448918.920720: Worker 0: training step 632 done (global step: 951)\n",
      "1507448918.924355: Worker 0: training step 633 done (global step: 953)\n",
      "1507448918.928487: Worker 0: training step 634 done (global step: 954)\n",
      "1507448918.932513: Worker 0: training step 635 done (global step: 956)\n",
      "1507448918.936041: Worker 0: training step 636 done (global step: 958)\n",
      "1507448918.939429: Worker 0: training step 637 done (global step: 959)\n",
      "1507448918.942928: Worker 0: training step 638 done (global step: 961)\n",
      "1507448918.946363: Worker 0: training step 639 done (global step: 962)\n",
      "1507448918.950068: Worker 0: training step 640 done (global step: 964)\n",
      "1507448918.954095: Worker 0: training step 641 done (global step: 966)\n",
      "1507448918.958177: Worker 0: training step 642 done (global step: 967)\n",
      "1507448918.961964: Worker 0: training step 643 done (global step: 969)\n",
      "1507448918.966039: Worker 0: training step 644 done (global step: 971)\n",
      "1507448918.969841: Worker 0: training step 645 done (global step: 973)\n",
      "1507448918.973549: Worker 0: training step 646 done (global step: 974)\n",
      "1507448918.977268: Worker 0: training step 647 done (global step: 976)\n",
      "1507448918.981608: Worker 0: training step 648 done (global step: 977)\n",
      "1507448918.985267: Worker 0: training step 649 done (global step: 979)\n",
      "1507448918.989505: Worker 0: training step 650 done (global step: 980)\n",
      "1507448918.993339: Worker 0: training step 651 done (global step: 982)\n",
      "1507448918.997618: Worker 0: training step 652 done (global step: 984)\n",
      "1507448919.001108: Worker 0: training step 653 done (global step: 986)\n",
      "1507448919.005405: Worker 0: training step 654 done (global step: 987)\n",
      "1507448919.009507: Worker 0: training step 655 done (global step: 989)\n",
      "1507448919.013564: Worker 0: training step 656 done (global step: 991)\n",
      "1507448919.017172: Worker 0: training step 657 done (global step: 993)\n",
      "1507448919.023714: Worker 0: training step 658 done (global step: 994)\n",
      "1507448919.029139: Worker 0: training step 659 done (global step: 996)\n",
      "1507448919.032630: Worker 0: training step 660 done (global step: 997)\n",
      "1507448919.036010: Worker 0: training step 661 done (global step: 999)\n",
      "1507448919.039764: Worker 0: training step 662 done (global step: 1000)\n",
      "Training ends @ 1507448919.039794\n",
      "Training elapsed time: 3.504209 s\n",
      "After 1000 training step(s), validation cross entropy = 503.528\n",
      "Job state: succeeded ExitCode: 0\n"
     ]
    }
   ],
   "source": [
    "utilities.wait_for_job_completion(client, cfg.resource_group, job_name, cluster_name, 'stdouterr', 'stdout-wk-0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download stdout.txt and stderr.txt files for the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://batchaisamples.file.core.windows.net/batchaisample/62254d4a-9a33-42b7-b57b-c3187d4d282e/batchaitests/jobs/tf_10_08_2017_074801/cbf33bd6-594f-4181-b2f9-06a06288f924/stderr-ps-0.txt?sv=2016-05-31&sr=f&sig=GvBgyrvVJ6pyrqLDIdvXBaVhl%2F2%2Fhyug58knTOMtu4U%3D&se=2017-10-08T08%3A49%3A56Z&sp=rl ...Done\n",
      "Downloading https://batchaisamples.file.core.windows.net/batchaisample/62254d4a-9a33-42b7-b57b-c3187d4d282e/batchaitests/jobs/tf_10_08_2017_074801/cbf33bd6-594f-4181-b2f9-06a06288f924/stderr-wk-0.txt?sv=2016-05-31&sr=f&sig=1pMqHHci2CjHZQpJyfh%2B1wQ2aSN%2BBymW%2F2Sh0rrXDzU%3D&se=2017-10-08T08%3A49%3A56Z&sp=rl ...Done\n",
      "Downloading https://batchaisamples.file.core.windows.net/batchaisample/62254d4a-9a33-42b7-b57b-c3187d4d282e/batchaitests/jobs/tf_10_08_2017_074801/cbf33bd6-594f-4181-b2f9-06a06288f924/stderr-wk-1.txt?sv=2016-05-31&sr=f&sig=HVmkIq3BQQTCzkV6MjMo9s%2FDXIo57FPKbOUzsiwotlw%3D&se=2017-10-08T08%3A49%3A56Z&sp=rl ...Done\n",
      "Downloading https://batchaisamples.file.core.windows.net/batchaisample/62254d4a-9a33-42b7-b57b-c3187d4d282e/batchaitests/jobs/tf_10_08_2017_074801/cbf33bd6-594f-4181-b2f9-06a06288f924/stdout-ps-0.txt?sv=2016-05-31&sr=f&sig=D%2FGJ%2BfjjXgzWL8FwmlbGSzkiZrwMRc2uRfWLG2SWG4I%3D&se=2017-10-08T08%3A49%3A56Z&sp=rl ...Done\n",
      "Downloading https://batchaisamples.file.core.windows.net/batchaisample/62254d4a-9a33-42b7-b57b-c3187d4d282e/batchaitests/jobs/tf_10_08_2017_074801/cbf33bd6-594f-4181-b2f9-06a06288f924/stdout-wk-0.txt?sv=2016-05-31&sr=f&sig=t9ESjN%2Bv%2B%2BxzUbhp6Eivlh%2BD5Xq1AzqvDYZ1x2EYuiQ%3D&se=2017-10-08T08%3A49%3A56Z&sp=rl ...Done\n",
      "Downloading https://batchaisamples.file.core.windows.net/batchaisample/62254d4a-9a33-42b7-b57b-c3187d4d282e/batchaitests/jobs/tf_10_08_2017_074801/cbf33bd6-594f-4181-b2f9-06a06288f924/stdout-wk-1.txt?sv=2016-05-31&sr=f&sig=C9ch7XoFW5FKaou9icdEpQBtx0e1PDVx18XJWP59yks%3D&se=2017-10-08T08%3A49%3A56Z&sp=rl ...Done\n",
      "All files Downloaded\n"
     ]
    }
   ],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, job_name, models.JobsListOutputFilesOptions(\"stdOuterr\")) \n",
    "for file in list(files):\n",
    "    utilities.download_file(file.download_url, file.name)\n",
    "print(\"All files Downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout-wk-0.txt content:\n",
      "2017-10-08 07:48:34.523318: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:34.528879: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:34.536706: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:34.545402: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:34.554013: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:34.901732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: Tesla K80\n",
      "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
      "pciBusID f81b:00:00.0\n",
      "Total memory: 11.17GiB\n",
      "Free memory: 11.11GiB\n",
      "2017-10-08 07:48:34.966547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n",
      "2017-10-08 07:48:34.986492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n",
      "2017-10-08 07:48:35.000723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: f81b:00:00.0)\n",
      "2017-10-08 07:48:35.100625: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> 10.0.0.5:2222}\n",
      "2017-10-08 07:48:35.112441: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> 10.0.0.4:2222}\n",
      "2017-10-08 07:48:35.144256: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2223\n",
      "2017-10-08 07:48:35.318164: I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session a6afb3482c24bfe4 with config: \n",
      "device_filters: \"/job:ps\"\n",
      "device_filters: \"/job:worker/task:0\"\n",
      "allow_soft_placement: true\n",
      "\n",
      "\n",
      "stdout-wk-1.txt content:\n",
      "Warning: Permanently added '[10.0.0.4]:23' (ECDSA) to the list of known hosts.\n",
      "2017-10-08 07:48:30.390012: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:30.390066: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:30.390075: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:30.390081: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:30.390087: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-10-08 07:48:30.731190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \n",
      "name: Tesla K80\n",
      "major: 3 minor: 7 memoryClockRate (GHz) 0.8235\n",
      "pciBusID bc4d:00:00.0\n",
      "Total memory: 11.92GiB\n",
      "Free memory: 11.86GiB\n",
      "2017-10-08 07:48:30.731232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n",
      "2017-10-08 07:48:30.731242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n",
      "2017-10-08 07:48:30.731258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: bc4d:00:00.0)\n",
      "2017-10-08 07:48:30.801987: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job ps -> {0 -> 10.0.0.5:2222}\n",
      "2017-10-08 07:48:30.802034: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.0.5:2223, 1 -> localhost:2222}\n",
      "2017-10-08 07:48:30.803714: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2222\n",
      "2017-10-08 07:48:36.604864: I tensorflow/core/distributed_runtime/master_session.cc:1012] Start master session 9e87573b24665fe7 with config: \n",
      "device_filters: \"/job:ps\"\n",
      "device_filters: \"/job:worker/task:1\"\n",
      "allow_soft_placement: true\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(nodes_count):\n",
    "    print('stdout-wk-{0}.txt content:'.format(n))\n",
    "    with open('stderr-wk-{0}.txt'.format(n)) as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client.jobs.delete(cfg.resource_group, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client.clusters.delete(cfg.resource_group, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
